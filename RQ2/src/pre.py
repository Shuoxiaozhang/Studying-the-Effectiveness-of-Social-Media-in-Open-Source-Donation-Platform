import pandas as pd
import requests
import json
import time
import concurrent.futures

# GraphQL ç«¯ç‚¹
url = "https://api.opencollective.com/graphql/v2"

# ä½ çš„ä¸ªäººä»¤ç‰Œ
personal_token = "f2e80a439066362a24289599eaf145349cc03c5a"

# è¯·æ±‚å¤´
headers = {
    "Content-Type": "application/json",
    "Personal-Token": personal_token
}

# ä»£ç†è®¾ç½®ï¼ˆæ ¹æ®ä½ çš„ä»£ç†å·¥å…·ä¿®æ”¹ç«¯å£ï¼‰
proxies = {
    "http": "http://127.0.0.1:12334",  # HTTP ä»£ç†
    "https": "http://127.0.0.1:12334",
    # å¦‚æœä½¿ç”¨ SOCKS5 ä»£ç†ï¼Œæ”¹ä¸ºï¼š
    # "http": "socks5h://127.0.0.1:7897",
    # "https": "socks5h://127.0.0.1:7897",
}

# è¯»å– cleaned_project_slugs.csv
slugs_df = pd.read_csv('E:/bishe/newdata/collectiveinfo/filtered_slugs.csv')

# ä»ç¬¬å¤šå°‘è¡Œå¼€å§‹çˆ¬å–
start_index = 0
slugs_to_query = slugs_df.iloc[start_index:]['slug']

# GraphQL æŸ¥è¯¢
query = """
query($slug: String) {
  collective(slug: $slug) {
    slug
    name
    type
    tags
    categories
    orders {
        totalCount
    }
    stats {
        contributionsCount
        contributorsCount
    }
    totalFinancialContributors
    location{
        country
    }
  }
}
"""

# æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„ slug æ•°é‡
batch_size = 100

# å°† slug åˆ’åˆ†ä¸ºå¤šä¸ªæ‰¹æ¬¡
def chunk_slugs(slugs, batch_size):
    for i in range(0, len(slugs), batch_size):
        yield slugs[i:i + batch_size]

# å¹¶è¡Œçˆ¬å–æ•°æ®
def fetch_data(batch_slugs):
    batch_data = []
    failed_slugs = []
    for slug in batch_slugs:
        variables = {"slug": slug}
        retries = 0
        while retries < 5:  # æœ€å¤šé‡è¯• 5 æ¬¡
            try:
                response = requests.post(
                    url,
                    json={"query": query, "variables": variables},
                    headers=headers,
                    proxies=proxies,  # ä½¿ç”¨ä»£ç†
                    timeout=10  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ— é™ç­‰å¾…
                )

                if response.status_code == 200:
                    data = response.json()

                    # æ£€æŸ¥ API æ˜¯å¦è¿”å›é”™è¯¯
                    if 'errors' in data:
                        print(f"âŒ é”™è¯¯: {slug} - {data['errors']}")
                        failed_slugs.append(slug)
                    else:
                        batch_data.append(data)
                        print(f"âœ… æˆåŠŸ: {slug}")
                    break  # è¯·æ±‚æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                elif response.status_code == 429:
                    print(f"âš ï¸ è¯·æ±‚è¿‡å¤š (429)ï¼Œç­‰å¾… 10 ç§’åé‡è¯•...")
                    time.sleep(10)
                    retries += 1

                elif response.status_code == 403:
                    print(f"ğŸš« è®¿é—®è¢«æ‹’ç» (403) - {slug}ï¼Œå¯èƒ½æ˜¯ API é™åˆ¶æˆ–ä»£ç†é—®é¢˜")
                    failed_slugs.append(slug)
                    break

                else:
                    print(f"â— è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code} - {slug}")
                    failed_slugs.append(slug)
                    break

            except requests.exceptions.RequestException as e:
                print(f"ğŸŒ è¯·æ±‚å¼‚å¸¸ï¼Œé”™è¯¯: {e}")
                failed_slugs.append(slug)
                break

    return batch_data, failed_slugs

# ä¿å­˜æŸ¥è¯¢æˆåŠŸçš„ç»“æœåˆ° JSON æ–‡ä»¶
def save_to_file(batch_data, filename):
    with open(filename, 'a', encoding='utf-8') as file:
        if batch_data:
            for data in batch_data:
                json.dump(data, file, ensure_ascii=False, indent=4)
                file.write("\n")

# æ‰“å¼€æ–‡ä»¶ï¼Œå‡†å¤‡ä¿å­˜æŸ¥è¯¢æˆåŠŸçš„ç»“æœ
success_file = 'E:/bishe/newdata/collectiveinfo/success_collective_data_lost.json'
failed_file = 'E:/bishe/newdata/collectiveinfo/failed_slugs_lost.csv'

with open(success_file, 'w', encoding='utf-8') as f:
    f.write('[')  # JSON æ•°ç»„èµ·å§‹

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œçˆ¬å–æ•°æ®
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:  # é™åˆ¶çº¿ç¨‹æ•°ä¸º 2
        slugs_batches = list(chunk_slugs(slugs_to_query, batch_size))
        futures = [executor.submit(fetch_data, batch) for batch in slugs_batches]

        # è·å–çº¿ç¨‹ç»“æœå¹¶ä¿å­˜
        all_failed_slugs = []
        for future in concurrent.futures.as_completed(futures):
            batch_data, failed_slugs = future.result()
            save_to_file(batch_data, success_file)
            all_failed_slugs.extend(failed_slugs)
            print(f"ğŸ“¦ å¤„ç†å®Œä¸€æ‰¹æ•°æ®")

    # ç»“æŸ JSON æ•°ç»„
    with open(success_file, 'a', encoding='utf-8') as f:
        f.write(']')

# ä¿å­˜æŸ¥è¯¢å¤±è´¥çš„ slug åˆ° CSV æ–‡ä»¶
failed_slugs_df = pd.DataFrame(all_failed_slugs, columns=['Failed Slugs'])
failed_slugs_df.to_csv(failed_file, index=False)

print(f"ğŸ“‚ æŸ¥è¯¢å¤±è´¥çš„ slug å·²ä¿å­˜åˆ° {failed_file}")
